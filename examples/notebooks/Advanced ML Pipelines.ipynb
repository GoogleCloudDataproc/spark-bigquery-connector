{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "<h3>Problem: As a PM, I write lots of blogs. How do I know if they will be received well by  readers?</h3>"}, {"cell_type": "markdown", "metadata": {}, "source": "<table>\n    <tr>\n        <td><img src=\"https://jayclouse.com/wp-content/uploads/2019/06/hacker_news.webp\" height=300 width=300></img></td>\n        <td><img src=\"https://miro.medium.com/max/852/1*wJ18DgYgtsscG63Sn56Oyw.png\" height=300 width=300></img></td>\n    </tr>\n</table>"}, {"cell_type": "markdown", "metadata": {}, "source": "<h1>Background on Spark ML</h1>"}, {"cell_type": "markdown", "metadata": {}, "source": "DataFrame: This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.\n\nTransformer: A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n\nEstimator: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n\nPipeline: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n\nParameter: All Transformers and Estimators now share a common API for specifying parameters."}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/html": "<img src=\"https://spark.apache.org/docs/3.0.0-preview/img/ml-Pipeline.png\"/>", "text/plain": "<IPython.core.display.Image object>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "from IPython.display import Image\nImage(url='https://spark.apache.org/docs/3.0.0-preview/img/ml-Pipeline.png') "}, {"cell_type": "markdown", "metadata": {}, "source": "<h2>Loading Hackernews Text From BigQuery</h2>"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nscala_minor_version = str(spark.sparkContext._jvm.scala.util.Properties.versionString().replace(\"version \",\"\").split('.')[1])\nspark = SparkSession.builder.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.\" + scala_minor_version + \":0.18.0\") \\\n                                    .enableHiveSupport() \\\n                                    .getOrCreate()"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "df = spark.read \\\n  .format(\"bigquery\") \\\n  .load(\"google.com:crosbie-test-project.demos.hackernewssample\")"}, {"cell_type": "code", "execution_count": 84, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+-----------------+--------------------+\n|summary|            score|                text|\n+-------+-----------------+--------------------+\n|  count|             1000|                1000|\n|   mean|            8.188|                null|\n| stddev|44.01315500863138|                null|\n|    min|                1|\"Do you have a qu...|\n|    max|             1081|\u0647\u064a \u0634\u0631\u0643\u0629 \u0645\u062a\u062e\u0635\u0635\u0629 \u0641\u064a...|\n+-------+-----------------+--------------------+\n\n"}], "source": "df.describe().show()"}, {"cell_type": "markdown", "metadata": {}, "source": "<h2>Prepare the data using Spark SQL<h2>\n<h4>Create a random ID to distribute between test and training sets</h4>\n<h4>Make the score a binary variable so we can run a logicistic regression model on it</h4>"}, {"cell_type": "code", "execution_count": 86, "metadata": {}, "outputs": [], "source": "df.registerTempTable(\"df\")\nfrom pyspark.sql import functions as F\ndf_full = spark.sql(\"select cast(round(rand() * 100) as int) as id, text, case when score > 10 THEN 1.0 else 0.0 end as label from df\")"}, {"cell_type": "code", "execution_count": 87, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----+\n| id|count|\n+---+-----+\n| 22|   17|\n| 39|   16|\n| 25|   15|\n| 55|   15|\n| 23|   15|\n| 47|   15|\n| 38|   15|\n| 71|   14|\n|  5|   14|\n| 98|   14|\n| 32|   14|\n| 44|   13|\n| 43|   13|\n| 59|   13|\n| 85|   13|\n| 83|   13|\n| 24|   13|\n| 82|   13|\n| 58|   13|\n| 53|   13|\n+---+-----+\nonly showing top 20 rows\n\n"}], "source": "df_full.groupby('id').count().sort('count', ascending=False).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "<h4>Create our training and test sets</h4>"}, {"cell_type": "code", "execution_count": 95, "metadata": {}, "outputs": [], "source": "#use the above table to identify ~10% holdback for test\nholdback = \"(22,39,25,55,23,47,38,71,5,98)\""}, {"cell_type": "code", "execution_count": 115, "metadata": {}, "outputs": [], "source": "#create test set by dropping label \ndf_test = df_full.where(\"id in {}\".format(holdback))\ndf_test = df_test.drop(\"label\")\nrdd_test = df_test.rdd\ntest = rdd_test.map(tuple)\ntesting = spark.createDataFrame(test,[\"id\", \"text\"])"}, {"cell_type": "code", "execution_count": 99, "metadata": {}, "outputs": [], "source": "#training data - Spark ML is expecting tuples so convert to RDD to map back to tuples (may not be required)\ndf_train = df_full.where(\"id not in {}\".format(holdback))\nrdd_train = df_train.rdd\ntrain = rdd_train.map(tuple)\ntraining = spark.createDataFrame(train,[\"id\", \"text\", \"label\"])"}, {"cell_type": "code", "execution_count": 114, "metadata": {}, "outputs": [{"data": {"text/plain": "91"}, "execution_count": 114, "metadata": {}, "output_type": "execute_result"}], "source": "#a little less than 10% of the trainig data is positively reviewed. Should be okay. \ntraining.where(\"label > 0\").count()"}, {"cell_type": "markdown", "metadata": {}, "source": "<h2>Build our ML Pipeline</h2>"}, {"cell_type": "markdown", "metadata": {}, "source": "<h3>Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.</h3>"}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer"}, {"cell_type": "code", "execution_count": 116, "metadata": {}, "outputs": [], "source": "\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n\nlr = LogisticRegression(maxIter=10, regParam=0.001)\n\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"}, {"cell_type": "code", "execution_count": 117, "metadata": {}, "outputs": [], "source": "# Fit the pipeline to hacker news articles\nmodel = pipeline.fit(training)"}, {"cell_type": "markdown", "metadata": {}, "source": "<h3>Review model based on test set</h3>"}, {"cell_type": "code", "execution_count": 119, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "(22, Other) --> prob=[0.3819756099050897,0.6180243900949102], prediction=1.000000\n(55, While we may talk often about other pieces of the software development lifecycle, shell scripts don&#x27;t usually get to take center stage. Let&#x27;s share some creative little scripts we&#x27;ve come up with over the years!) --> prob=[0.4063985380451119,0.5936014619548882], prediction=1.000000\n"}], "source": "# Make predictions on test documents and print columns of interest.\nprediction = model.transform(testing)\nselected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\").where(\"prediction > 0\")\nfor row in selected.collect():\n    rid, text, prob, prediction = row\n    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"}, {"cell_type": "markdown", "metadata": {}, "source": "<h2>Use the model to decide which PM blog to use</h2>"}, {"cell_type": "code", "execution_count": 122, "metadata": {}, "outputs": [], "source": "my_blog = \"\"\"\nThe life of a data scientist can be challenging. If you\u2019re in this role, your job may involve anything from understanding the day-to-day business behind the data to keeping up with the latest machine learning academic research. With all that a data scientist must do to be effective, you shouldn\u2019t have to worry about migrating data environments or dealing with processing limitations associated with working with raw data. \n\nGoogle Cloud\u2019s Dataproc lets you run cloud-native Apache Spark and Hadoop clusters easily. This is especially helpful as data growth relocates data scientists and machine learning researchers from personal servers and laptops into distributed cluster environments like Apache Spark, which offers Python and R interfaces for data of any size. You can run open source data processing on Google Cloud, making Dataproc one of the fastest ways to extend your existing data analysis to cloud-sized datasets.  \n\nWe\u2019re announcing the general availability of several new Dataproc features that will let you apply the open source tools, algorithms, and programming languages that you use today to large datasets. This can be done without having to manage clusters and computers. These new GA features make it possible for data scientists and analysts to build production systems based on personalized development environments. \n\"\"\"\n\npmm_blog = \"\"\"\nDataproc makes open source data and analytics processing fast, easy, and more secure in the cloud.\n\nNew customers get $300 in free credits to spend on Dataproc or other Google Cloud products during the first 90 days. \n\nGo to console\nSpin up an autoscaling cluster in 90 seconds on custom machines\nBuild fully managed Apache Spark, Apache Hadoop, Presto, and other OSS clusters\nOnly pay for the resources you use and lower the total cost of ownership of OSS\nEncryption and unified security built into every cluster\nAccelerate data science with purpose-built clusters\n\"\"\"\n\n\nboss_blog = \"\"\"\n\nIn 2014, we made a decision to build our core data platform on Google Cloud Platform and one of the products which was critical for the decision was Google BigQuery. The scale at which it enabled us to perform analysis we knew would be critical in long run for our business. Today we have more than 200 unique users performing analysis on a monthly basis.\n\nOnce we started using Google BiqQuery at scale we soon realized our analysts needed better tooling around it. The key requests we started getting were\n\nAbility to schedule jobs: Analysts needed to have ability to run queries at regular intervals to generate data and metrics.\nDefine workflow of queries: Basically analysts wanted to run multiple queries in a sequence and share data across them through temp tables.\nSimplified data sharing: Finally it became clear teams needed to share this data generated with other systems. For example download it to leverage in R programs or send it to another system to process through Kafka.\n\n\"\"\""}, {"cell_type": "code", "execution_count": 129, "metadata": {}, "outputs": [], "source": "pm_blog_off = spark.createDataFrame([\n    ('me', my_blog),\n    ('pmm', pmm_blog),\n    ('sudhir', boss_blog)\n], [\"id\", \"text\"])"}, {"cell_type": "code", "execution_count": 130, "metadata": {}, "outputs": [], "source": "blog_prediction = model.transform(pm_blog_off)"}, {"cell_type": "code", "execution_count": 131, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----------+\n|    id|prediction|\n+------+----------+\n|    me|       0.0|\n|   pmm|       1.0|\n|sudhir|       0.0|\n+------+----------+\n\n"}], "source": "blog_prediction.select(\"id\",\"prediction\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "<h2>Save our trained model to GCS</h2>"}, {"cell_type": "code", "execution_count": 137, "metadata": {}, "outputs": [], "source": "model.save(\"gs://crosbie-dev/blog-validation-model\")"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 4}