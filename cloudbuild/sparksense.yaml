steps:
  # 1. Create a Docker image containing hadoop-connectors repo
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-nightly', '-f', 'cloudbuild/Dockerfile', '.']

  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    id: 'sparksense-v1-20-bq'
    args: ['/workspace/cloudbuild/sparksense.sh',
           'spark-lib-nightly-snapshots', #connector bucket
           'V1_20_BQ', #runId
           'spark-sense-c2d-v1-20-bq', #cluster
           'us-central1', #cluster-region
           'tpcds', #bechmark
           'spark-bigquery-with-dependencies_2.12-nightly-snapshot.jar', #BQ jar
           'bq', #runType
           'tpcds_1T_partitioned_gcs', #database
    ]
    waitFor: ['docker-build']

  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    id: 'sparksense-v2-20-bq'
    args: ['/workspace/cloudbuild/sparksense.sh',
           'spark-lib-nightly-snapshots', #connector bucket
           'V2_20_BQ', #runId
           'spark-sense-c2d-v2-20-bq', #cluster
           'us-central1', #cluster-region
           'tpcds', #bechmark
           'spark-3.1-bigquery-nightly-snapshot-preview.jar', #BQ jar
           'bq', #runType
           'tpcds_1T_partitioned_gcs', #database
    ]
    waitFor: ['docker-build']

  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    id: 'sparksense-gcs-20'
    args: ['/workspace/cloudbuild/sparksense.sh',
           'spark-lib-nightly-snapshots', #connector bucket
           'GCS_20', #runId
           'spark-sense-c2d-20-gcs', #cluster
           'us-central1', #cluster-region
           'tpcds', #bechmark
           'spark-bigquery-with-dependencies_2.12-nightly-snapshot.jar', #BQ jar
           'gcs', #runType
           'tpcds_sf1000', #database
    ]
    waitFor: ['docker-build']

# TODO - uncomment this once everything works fine for image 2.1
#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-v1-21-bq'
#    args: ['/workspace/cloudbuild/sparksense.sh',
#           'spark-lib-nightly-snapshots', #connector bucket
#           'V1_21_BQ', #runId
#           'spark-sense-c2d-v1-21-bq', #cluster
#           'us-central1', #cluster-region
#           'tpcds', #bechmark
#           'spark-bigquery-with-dependencies_2.12-nightly-snapshot.jar', #BQ jar
#           'bq', #runType
#           'tpcds_1T_partitioned_gcs', #database
#    ]
#    waitFor: ['docker-build']
#
#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-v2-21-bq'
#    args: ['/workspace/cloudbuild/sparksense.sh',
#           'spark-lib-nightly-snapshots', #connector bucket
#           'V2_21_BQ', #runId
#           'spark-sense-c2d-v2-21-bq', #cluster
#           'us-central1', #cluster-region
#           'tpcds', #bechmark
#           'spark-3.1-bigquery-nightly-snapshot-preview.jar', #BQ jar
#           'bq', #runType
#           'tpcds_1T_partitioned_gcs', #database
#    ]
#    waitFor: ['docker-build']
#
#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-gcs-21'
#    args: ['/workspace/cloudbuild/sparksense.sh',
#           'spark-lib-nightly-snapshots', #connector bucket
#           'GCS_21', #runId
#           'spark-sense-c2d-21-gcs', #cluster
#           'us-central1', #cluster-region
#           'tpcds', #bechmark
#           'spark-bigquery-with-dependencies_2.12-nightly-snapshot.jar', #BQ jar
#           'gcs', #runType
#           'tpcds_sf1000', #database
#    ]
#    waitFor: ['docker-build']

logsBucket: 'gs://suryasoma-test-cicd'
timeout: 86400s
