steps:
  # 1. Create a Docker image containing hadoop-connectors repo
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-nightly', '-f', 'cloudbuild/Dockerfile', '.']

  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    id: 'sparksense-v1-20-bq'
    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots','v1-20-bq']
    waitFor: ['docker-build']

  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    id: 'sparksense-v2-20-bq'
    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots', 'v2-20-bq']
    waitFor: ['docker-build']

  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    id: 'sparksense-gcs-20'
    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots', 'gcs-20']
    waitFor: ['docker-build']

#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-v1-21-bq'
#    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots','v1-21-bq']
#    waitFor: ['docker-build']
#
#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-v2-21-bq'
#    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots', 'v2-21-bq']
#    waitFor: ['docker-build']
#
#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-gcs-21'
#    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots', 'gcs-21']
#    waitFor: ['docker-build']

logsBucket: 'gs://suryasoma-test-cicd'
timeout: 86400s
