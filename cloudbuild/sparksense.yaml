steps:
  # 1. Create a Docker image containing hadoop-connectors repo
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-nightly', '-f', 'cloudbuild/Dockerfile', '.']

  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: 'bash'
    id: 'sparksense-v1-20-bq'
    args: ['/workspace/cloudbuild/sparksense.sh',
           'spark-lib-nightly-snapshots', #connector bucket
           'V1_20_BQ', #runId
           'spark-sense-c2d-v1-20-bq', #cluster
           'us-central1', #cluster-region
           'tpcds', #bechmark
           'spark-bigquery-with-dependencies_2.12-nightly-snapshot-sparksense.jar', #BQ jar
           'bq', #runType
           'tpcds_1T_partitioned_gcs', #database
    ]
    waitFor: ['docker-build']

#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-v2-20-bq'
#    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots', 'v2-20-bq']
#    waitFor: ['docker-build']
#
#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-gcs-20'
#    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots', 'gcs-20']
#    waitFor: ['docker-build']

#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-v1-21-bq'
#    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots','v1-21-bq']
#    waitFor: ['docker-build']
#
#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-v2-21-bq'
#    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots', 'v2-21-bq']
#    waitFor: ['docker-build']
#
#  - name: 'gcr.io/cloud-builders/gcloud'
#    entrypoint: 'bash'
#    id: 'sparksense-gcs-21'
#    args: ['/workspace/cloudbuild/sparksense.sh', 'spark-lib-nightly-snapshots', 'gcs-21']
#    waitFor: ['docker-build']

logsBucket: 'gs://suryasoma-test-cicd'
timeout: 86400s
