steps:
  # 1. Create a Docker image containing hadoop-connectors repo
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-nightly', '-f', 'cloudbuild/Dockerfile.nightly', '.']

# 2. Fetch maven and dependencies
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-nightly'
    id: 'maven-build'
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/nightly.sh', 'build']
    env:
      - 'GOOGLE_CLOUD_PROJECT=${_GOOGLE_CLOUD_PROJECT}'
      - 'TEMPORARY_GCS_BUCKET=${_TEMPORARY_GCS_BUCKET}'
      - 'ACCEPTANCE_TEST_BUCKET=${_ACCEPTANCE_TEST_BUCKET}'
      - 'SERVERLESS_NETWORK_URI=${_SERVERLESS_NETWORK_URI}'
      - 'BIGLAKE_CONNECTION_ID=${_BIGLAKE_CONNECTION_ID}'
      - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-nightly'
    id: 'copy-to-gcs'
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/nightly.sh', 'copy-to-gcs']
# Trigger to start spark-sense
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'spark-sense-trigger'
    entrypoint: 'bash'
    args:
    - '-c'
    - |
        gcloud pubsub topics publish spark-sense-nightly --message="trigger spark-sense run" 

  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build-presubmit-base'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit-base', '-f', 'cloudbuild/Dockerfile.presubmit-base', '.']

  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-push-presubmit-base'
    args: ['push', 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit-base:latest']

# Many tests, so it takes time
timeout: 10800s

options:
  machineType: 'N1_HIGHCPU_32'
