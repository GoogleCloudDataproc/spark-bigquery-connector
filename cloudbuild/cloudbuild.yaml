steps:
  # 1. Create a Docker image containing hadoop-connectors repo
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit', '-f', 'cloudbuild/Dockerfile', '.']

# 2. Fetch maven and dependencies
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit'
    id: 'init'
    waitFor: ['docker-build']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'init']
    env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'

# 3. Run unit tests
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit'
    id: 'unit-tests'
    waitFor: ['init']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'unittest']
    env:
    - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'

# 4a. Run integration tests concurrently with unit tests (DSv1, Scala 2.12)
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit'
    id: 'integration-tests-2.12'
    waitFor: ['unit-tests']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'integrationtest-2.12']
    env:
      - 'GOOGLE_CLOUD_PROJECT=${_GOOGLE_CLOUD_PROJECT}'
      - 'TEMPORARY_GCS_BUCKET=${_TEMPORARY_GCS_BUCKET}'
      - 'BIGLAKE_CONNECTION_ID=${_BIGLAKE_CONNECTION_ID}'

# 4b. Run integration tests concurrently with unit tests (DSv1, Scala 2.13)
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit'
    id: 'integration-tests-2.13'
    waitFor: ['integration-tests-2.12']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'integrationtest-2.13']
    env:
    - 'GOOGLE_CLOUD_PROJECT=${_GOOGLE_CLOUD_PROJECT}'
    - 'TEMPORARY_GCS_BUCKET=${_TEMPORARY_GCS_BUCKET}'
    - 'BIGLAKE_CONNECTION_ID=${_BIGLAKE_CONNECTION_ID}'

# 4c. Run integration tests concurrently with unit tests (DSv2, Spark 2.4)
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit'
    id: 'integration-tests-2.4'
    waitFor: ['unit-tests']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'integrationtest-2.4']
    env:
    - 'GOOGLE_CLOUD_PROJECT=${_GOOGLE_CLOUD_PROJECT}'
    - 'TEMPORARY_GCS_BUCKET=${_TEMPORARY_GCS_BUCKET}'
    - 'BIGLAKE_CONNECTION_ID=${_BIGLAKE_CONNECTION_ID}'

# 4d. Run integration tests concurrently with unit tests (DSv2, Spark 3.1)
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit'
    id: 'integration-tests-3.1'
    waitFor: ['integration-tests-2.4']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'integrationtest-3.1']
    env:
      - 'GOOGLE_CLOUD_PROJECT=${_GOOGLE_CLOUD_PROJECT}'
      - 'TEMPORARY_GCS_BUCKET=${_TEMPORARY_GCS_BUCKET}'
      - 'BIGLAKE_CONNECTION_ID=${_BIGLAKE_CONNECTION_ID}'

# 5. Upload coverage to CodeCov
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-bigquery-connector-presubmit'
    id: 'upload-it-to-codecov'
    waitFor: ['integration-tests-2.12','integration-tests-2.13','integration-tests-2.4','integration-tests-3.1']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'upload-it-to-codecov']
    env:
      - 'CODECOV_TOKEN=${_CODECOV_TOKEN}'


# Tests take around 30 mins in general.
timeout: 1800s

options:
  machineType: 'N1_HIGHCPU_32'
